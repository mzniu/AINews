"""
VentureBeatæ–‡ç« å®Œæ•´çˆ¬è™«
æŠ“å–æ–‡ç« å†…å®¹ã€å›¾ç‰‡å’Œå…¶ä»–å…ƒæ•°æ®
"""
import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin, urlparse
from pathlib import Path

class VentureBeatArticleCrawler:
    """VentureBeatæ–‡ç« çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_article_content(self, url):
        """è·å–æ–‡ç« å®Œæ•´å†…å®¹"""
        try:
            print(f"æ­£åœ¨æŠ“å–: {url}")
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ–‡ç« ä¿¡æ¯
                article_data = {
                    'url': url,
                    'title': self._extract_title(soup),
                    'author': self._extract_author(soup),
                    'publish_date': self._extract_publish_date(soup),
                    'content': self._extract_content(soup),
                    'images': self._extract_images(soup, url),
                    'tags': self._extract_tags(soup),
                    'summary': self._extract_summary(soup)
                }
                
                return article_data
            else:
                print(f"HTTPé”™è¯¯: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {e}")
            return None
    
    def _extract_title(self, soup):
        """æå–æ ‡é¢˜"""
        # VentureBeaté€šå¸¸ä½¿ç”¨h1æ ‡ç­¾
        title_elem = soup.find('h1')
        if title_elem:
            return title_elem.get_text(strip=True)
        return "æœªæ‰¾åˆ°æ ‡é¢˜"
    
    def _extract_author(self, soup):
        """æå–ä½œè€…ä¿¡æ¯"""
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        author_selectors = [
            '[class*="author"] a',
            '[rel="author"]',
            '.byline-author',
            '.author-name'
        ]
        
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                return author_elem.get_text(strip=True)
        return "æœªçŸ¥ä½œè€…"
    
    def _extract_publish_date(self, soup):
        """æå–å‘å¸ƒæ—¥æœŸ"""
        # æŸ¥æ‰¾timeæ ‡ç­¾æˆ–åŒ…å«æ—¥æœŸçš„å…ƒç´ 
        time_elem = soup.find('time')
        if time_elem and time_elem.has_attr('datetime'):
            return time_elem['datetime']
        
        # æŸ¥æ‰¾åŒ…å«æ—¥æœŸæ–‡æœ¬çš„å…ƒç´ 
        date_selectors = ['.publish-date', '.entry-date', '[class*="date"]']
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                return date_elem.get_text(strip=True)
        return "æœªçŸ¥æ—¥æœŸ"
    
    def _extract_content(self, soup):
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        # VentureBeatçš„å†…å®¹é€šå¸¸åœ¨ç‰¹å®šçš„divä¸­
        content_selectors = [
            '[class*="content"]',
            'article',
            '.post-content',
            '.entry-content',
            'main'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in content_elem.select('script, style, .ad, .advertisement, .related-posts, nav'):
                    unwanted.decompose()
                
                # è·å–çº¯æ–‡æœ¬å†…å®¹
                content_text = content_elem.get_text(separator='\n', strip=True)
                return content_text[:2000] + "..." if len(content_text) > 2000 else content_text
        
        return "æœªæ‰¾åˆ°æ–‡ç« å†…å®¹"
    
    def _extract_images(self, soup, base_url):
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        images = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
        img_elements = soup.find_all('img')
        
        for img in img_elements:
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                # å¤„ç†ç›¸å¯¹URL
                full_url = urljoin(base_url, src)
                alt_text = img.get('alt', '')
                
                images.append({
                    'url': full_url,
                    'alt': alt_text,
                    'title': img.get('title', '')
                })
        
        return images
    
    def _extract_tags(self, soup):
        """æå–æ ‡ç­¾"""
        tags = []
        tag_selectors = [
            '.tags a',
            '.post-tags a',
            '[rel="tag"]'
        ]
        
        for selector in tag_selectors:
            tag_elements = soup.select(selector)
            for tag in tag_elements:
                tag_text = tag.get_text(strip=True)
                if tag_text and tag_text not in tags:
                    tags.append(tag_text)
        
        return tags
    
    def _extract_summary(self, soup):
        """æå–æ–‡ç« æ‘˜è¦"""
        # æŸ¥æ‰¾meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        
        # æˆ–è€…ä»å†…å®¹ä¸­æå–å‰å‡ å¥è¯
        content = self._extract_content(soup)
        sentences = content.split('.')[:3]
        return '.'.join(sentences) + '.' if sentences else ""

def download_images(article_data, output_dir="downloaded_images"):
    """ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡"""
    if not article_data or not article_data.get('images'):
        print("æ²¡æœ‰å›¾ç‰‡éœ€è¦ä¸‹è½½")
        return []
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    images_dir = Path(output_dir) / "venturebeat"
    images_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded_images = []
    
    print(f"å¼€å§‹ä¸‹è½½ {len(article_data['images'])} å¼ å›¾ç‰‡...")
    
    for i, img_info in enumerate(article_data['images']):
        try:
            img_url = img_info['url']
            print(f"ä¸‹è½½å›¾ç‰‡ {i+1}/{len(article_data['images'])}: {img_url}")
            
            # å‘é€è¯·æ±‚ä¸‹è½½å›¾ç‰‡
            response = requests.get(img_url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }, timeout=30)
            
            if response.status_code == 200:
                # ç”Ÿæˆæ–‡ä»¶å
                parsed_url = urlparse(img_url)
                filename = f"image_{i+1:03d}_{os.path.basename(parsed_url.path)}"
                if not filename.endswith(('.jpg', '.png', '.gif', '.webp')):
                    filename += '.jpg'
                
                filepath = images_dir / filename
                
                # ä¿å­˜å›¾ç‰‡
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                downloaded_images.append(str(filepath))
                print(f"  âœ“ å·²ä¿å­˜: {filepath}")
            else:
                print(f"  âœ— ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            
        except Exception as e:
            print(f"  âœ— ä¸‹è½½å¼‚å¸¸: {e}")
            continue
    
    return downloaded_images

def main():
    """ä¸»å‡½æ•°"""
    url = "https://venturebeat.com/orchestration/new-agent-framework-matches-human-engineered-ai-systems-and-adds-zero"
    
    print("ğŸš€ å¼€å§‹æŠ“å–VentureBeatæ–‡ç« ")
    print("=" * 50)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = VentureBeatArticleCrawler()
    
    # æŠ“å–æ–‡ç« 
    article_data = crawler.get_article_content(url)
    
    if article_data:
        print("âœ… æ–‡ç« æŠ“å–æˆåŠŸ!")
        print(f"æ ‡é¢˜: {article_data['title']}")
        print(f"ä½œè€…: {article_data['author']}")
        print(f"å‘å¸ƒæ—¥æœŸ: {article_data['publish_date']}")
        print(f"å†…å®¹é•¿åº¦: {len(article_data['content'])} å­—ç¬¦")
        print(f"å›¾ç‰‡æ•°é‡: {len(article_data['images'])}")
        print(f"æ ‡ç­¾: {', '.join(article_data['tags'])}")
        print(f"æ‘˜è¦: {article_data['summary'][:100]}...")
        
        # ä¿å­˜æ–‡ç« æ•°æ®
        with open('venturebeat_article_full.json', 'w', encoding='utf-8') as f:
            json.dump(article_data, f, ensure_ascii=False, indent=2, default=str)
        print("ğŸ“ æ–‡ç« æ•°æ®å·²ä¿å­˜åˆ° venturebeat_article_full.json")
        
        # ä¸‹è½½å›¾ç‰‡
        downloaded_images = download_images(article_data)
        print(f"ğŸ–¼ï¸  æˆåŠŸä¸‹è½½ {len(downloaded_images)} å¼ å›¾ç‰‡")
        
        # æ›´æ–°æ–‡ç« æ•°æ®ä¸­çš„ä¸‹è½½å›¾ç‰‡è·¯å¾„
        article_data['downloaded_images'] = downloaded_images
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        with open('venturebeat_article_with_images.json', 'w', encoding='utf-8') as f:
            json.dump(article_data, f, ensure_ascii=False, indent=2, default=str)
        print("ğŸ’¾ å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ° venturebeat_article_with_images.json")
        
    else:
        print("âŒ æ–‡ç« æŠ“å–å¤±è´¥")

if __name__ == "__main__":
    main()