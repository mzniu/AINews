"""æµ‹è¯•çœŸå®çˆ¬å– - ä½¿ç”¨å¯è®¿é—®çš„ç½‘ç«™"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from src.crawlers.kr36_ai import Kr36AICrawler
from src.utils.logger import logger
import requests

def test_connection():
    """æµ‹è¯•ç½‘ç«™è¿æ¥"""
    url = "https://www.36kr.com"
    logger.info(f"æµ‹è¯•è¿æ¥: {url}")
    
    try:
        response = requests.get(url, timeout=5, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        logger.success(f"âœ… è¿æ¥æˆåŠŸ! çŠ¶æ€ç : {response.status_code}")
        return True
    except Exception as e:
        logger.error(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return False

def test_crawl():
    """æµ‹è¯•çˆ¬å–"""
    if not test_connection():
        logger.error("æ— æ³•è¿æ¥åˆ°ç½‘ç«™ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    logger.info("\n" + "=" * 60)
    logger.info("å¼€å§‹çˆ¬å–36æ°ªAIé¢‘é“")
    logger.info("=" * 60)
    
    crawler = Kr36AICrawler()
    
    try:
        articles = crawler.crawl_latest(max_articles=3)
        
        logger.success(f"\nâœ… æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
        
        for i, article in enumerate(articles, 1):
            logger.info(f"\n[{i}] {article.title}")
            logger.info(f"    URL: {article.url}")
            logger.info(f"    ä½œè€…: {article.author}")
            logger.info(f"    æ—¶é—´: {article.publish_time}")
            logger.info(f"    å†…å®¹é•¿åº¦: {len(article.content)} å­—ç¬¦")
            logger.info(f"    æ ‡ç­¾: {', '.join(article.tags)}")
        
        if articles:
            crawler.save_articles(articles)
            logger.success(f"\nğŸ’¾ æ–‡ç« å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_crawl()
