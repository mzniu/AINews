#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•36kré¡µé¢ç»“æ„ï¼Œåˆ†æå†…å®¹æå–é—®é¢˜
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from services.crawler_service import CrawlerService
import asyncio
from bs4 import BeautifulSoup

async def debug_36kr_structure():
    """è°ƒè¯•36kré¡µé¢ç»“æ„"""
    test_url = "https://www.36kr.com/p/3678583640810112"
    
    print("ğŸ” è°ƒè¯•36kré¡µé¢ç»“æ„")
    print("=" * 60)
    
    try:
        # 1. è·å–é¡µé¢å†…å®¹
        print("ğŸ“¥ è·å–é¡µé¢HTML...")
        html, title = await CrawlerService.get_page_content(test_url)
        print(f"ğŸ“„ æ ‡é¢˜: {title}")
        print(f"ğŸ“ HTMLé•¿åº¦: {len(html)} å­—ç¬¦")
        
        # 2. è§£æHTML
        soup = BeautifulSoup(html, 'lxml')
        
        # 3. å°è¯•å„ç§å¯èƒ½çš„å†…å®¹é€‰æ‹©å™¨
        selectors_to_try = [
            'article',
            '[class*="content"]',
            '[class*="article"]', 
            '[class*="post"]',
            '[id*="content"]',
            'main',
            '.main-content',
            '.article-content',
            '.post-content',
            '.content-wrapper',
            '[data-module-name*="article"]',
            '.kr-article-flow',
            '.article-detail',
            '.article-body',
            '.post-body'
        ]
        
        print(f"\nğŸ¯ æµ‹è¯•å„ç§å†…å®¹é€‰æ‹©å™¨:")
        for selector in selectors_to_try:
            elements = soup.select(selector)
            if elements:
                content_length = len(elements[0].get_text(strip=True))
                print(f"  âœ… {selector:25} -> {len(elements)}ä¸ªå…ƒç´ , å†…å®¹é•¿åº¦: {content_length}")
                
                # å¦‚æœå†…å®¹è¶³å¤Ÿé•¿ï¼Œæ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
                if content_length > 100:
                    text_preview = elements[0].get_text(strip=True)[:200]
                    print(f"     é¢„è§ˆ: {text_preview}...")
            else:
                print(f"  âŒ {selector:25} -> æœªæ‰¾åˆ°")
        
        # 4. æŸ¥æ‰¾å¯èƒ½çš„æ–‡ç« å®¹å™¨
        print(f"\nğŸ” æŸ¥æ‰¾å¯èƒ½çš„æ–‡ç« å®¹å™¨:")
        # æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬çš„div
        all_divs = soup.find_all('div')
        text_divs = []
        for div in all_divs:
            text_content = div.get_text(strip=True)
            if len(text_content) > 500:  # è¶…è¿‡500å­—ç¬¦çš„div
                text_divs.append((div, len(text_content)))
        
        # æŒ‰æ–‡æœ¬é•¿åº¦æ’åº
        text_divs.sort(key=lambda x: x[1], reverse=True)
        
        print(f"æ‰¾åˆ° {len(text_divs)} ä¸ªåŒ…å«é•¿æ–‡æœ¬çš„div:")
        for i, (div, length) in enumerate(text_divs[:5]):
            # è·å–divçš„classå’Œid
            classes = div.get('class', [])
            div_id = div.get('id', '')
            class_str = ' '.join(classes) if classes else 'æ— class'
            
            print(f"  {i+1}. é•¿åº¦: {length:5} å­—ç¬¦ | class: {class_str} | id: {div_id}")
            
            # æ˜¾ç¤ºå‰100å­—ç¬¦é¢„è§ˆ
            preview = div.get_text(strip=True)[:100]
            print(f"      é¢„è§ˆ: {preview}...")
            print()
            
        # 5. æ£€æŸ¥æ˜¯å¦æœ‰iframeæˆ–åŠ¨æ€åŠ è½½å†…å®¹
        iframes = soup.find_all('iframe')
        scripts = soup.find_all('script')
        
        print(f"ğŸ“Š é¡µé¢ç»“æ„åˆ†æ:")
        print(f"   iframeæ•°é‡: {len(iframes)}")
        print(f"   scriptæ ‡ç­¾æ•°é‡: {len(scripts)}")
        
        # æŸ¥æ‰¾å¯èƒ½çš„åŠ¨æ€å†…å®¹åŠ è½½
        dynamic_scripts = [s for s in scripts if s.get('src') and ('react' in s['src'].lower() or 'vue' in s['src'].lower() or 'app' in s['src'].lower())]
        print(f"   å¯èƒ½çš„æ¡†æ¶è„šæœ¬: {len(dynamic_scripts)}")
        for script in dynamic_scripts[:3]:
            print(f"     - {script.get('src', 'å†…è”è„šæœ¬')}")
            
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(debug_36kr_structure())