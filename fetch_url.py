"""
é€šç”¨ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·
ä½¿ç”¨æ–¹æ³•: python fetch_url.py <URL>
"""
import sys
import os
from pathlib import Path
from urllib.parse import urlparse, urljoin
from datetime import datetime
import json
import hashlib
import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from loguru import logger

# é…ç½®æ—¥å¿—
logger.add("logs/fetch_url_{time}.log", rotation="10 MB")


def get_page_content(url: str) -> tuple[str, str]:
    """ä½¿ç”¨Playwrightè·å–é¡µé¢HTMLå’Œæ ‡é¢˜"""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, wait_until='networkidle', timeout=30000)
            
            title = page.title()
            html = page.content()
            
            browser.close()
            logger.success(f"æˆåŠŸè·å–é¡µé¢: {title}")
            return html, title
    except Exception as e:
        logger.error(f"è·å–é¡µé¢å¤±è´¥: {e}")
        raise


def extract_content(html: str, base_url: str) -> dict:
    """æå–é¡µé¢å†…å®¹å’Œå›¾ç‰‡"""
    soup = BeautifulSoup(html, 'lxml')
    
    # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()
    
    # æå–æ­£æ–‡ï¼ˆå°è¯•å¤šç§é€‰æ‹©å™¨ï¼‰
    content_selectors = [
        'article',
        '[class*="content"]',
        '[class*="article"]',
        '[class*="post"]',
        '[id*="content"]',
        'main',
        '.main-content',
        'body'
    ]
    
    content_text = ""
    for selector in content_selectors:
        elements = soup.select(selector)
        if elements:
            content_text = elements[0].get_text(separator='\n', strip=True)
            if len(content_text) > 200:  # è‡³å°‘200å­—æ‰ç®—æœ‰æ•ˆå†…å®¹
                logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹: {selector}")
                break
    
    # æå–å›¾ç‰‡ï¼ˆæ ¹æ®ç½‘ç«™ç±»å‹é‡‡ç”¨ä¸åŒç­–ç•¥ï¼‰
    images = []
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºqbitaiç½‘ç«™
    parsed_url = urlparse(base_url)
    is_qbitai = 'qbitai.com' in parsed_url.netloc
    
    if is_qbitai:
        # æå–å…·æœ‰syl-page-imgç±»å’Œpgc-imgç±»çš„å›¾ç‰‡
        logger.info("æ£€æµ‹åˆ°qbitaiç½‘ç«™ï¼Œæå–syl-page-imgå’Œpgc-imgç±»çš„å›¾ç‰‡")
        
        # æå–syl-page-imgç±»å›¾ç‰‡
        syl_img_elements = soup.find_all('img', class_='syl-page-img')
        for img in syl_img_elements:
            src = img.get('src') or img.get('data-src') or img.get('data-original')
            if src:
                # è½¬æ¢ä¸ºç»å¯¹URL
                absolute_url = urljoin(base_url, src)
                images.append({
                    'url': absolute_url,
                    'alt': img.get('alt', ''),
                    'width': img.get('width'),
                    'height': img.get('height'),
                    'class': 'syl-page-img'
                })
        
        # æå–pgc-imgç±»å›¾ç‰‡ï¼ˆåœ¨pgc-img divå®¹å™¨å†…ï¼‰
        pgc_containers = soup.find_all('div', class_='pgc-img')
        for container in pgc_containers:
            img = container.find('img')
            if img:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    absolute_url = urljoin(base_url, src)
                    images.append({
                        'url': absolute_url,
                        'alt': img.get('alt', ''),
                        'width': img.get('width'),
                        'height': img.get('height'),
                        'class': 'pgc-img'
                    })
        
        logger.info(f"qbitaiç½‘ç«™æå–å®Œæˆ: syl-page-img {len(syl_img_elements)}å¼ , pgc-img {len(pgc_containers)}å¼ ")
    else:
        # å…¶ä»–ç½‘ç«™æå–æ‰€æœ‰å›¾ç‰‡
        logger.info("æå–é¡µé¢æ‰€æœ‰å›¾ç‰‡")
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-original')
            if src:
                # è½¬æ¢ä¸ºç»å¯¹URL
                absolute_url = urljoin(base_url, src)
                images.append({
                    'url': absolute_url,
                    'alt': img.get('alt', ''),
                    'width': img.get('width'),
                    'height': img.get('height')
                })
    
    logger.info(f"æå–åˆ° {len(images)} å¼ å›¾ç‰‡ (qbitaiæ¨¡å¼: {is_qbitai})")
    
    return {
        'content': content_text,
        'images': images
    }


def download_image(image_url: str, save_dir: Path, index: int) -> str:
    """ä¸‹è½½å•å¼ å›¾ç‰‡"""
    try:
        # ç”Ÿæˆæ–‡ä»¶å
        parsed = urlparse(image_url)
        ext = Path(parsed.path).suffix or '.jpg'
        filename = f"image_{index:03d}{ext}"
        filepath = save_dir / filename
        
        # ä¸‹è½½å›¾ç‰‡
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(image_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # ä¿å­˜å›¾ç‰‡
        with open(filepath, 'wb') as f:
            f.write(response.content)
        
        logger.success(f"ä¸‹è½½å›¾ç‰‡: {filename}")
        return str(filepath)
    except Exception as e:
        logger.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
        return ""


def save_results(url: str, title: str, content: str, images: list, output_dir: Path):
    """ä¿å­˜æŠ“å–ç»“æœ"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    save_dir = output_dir / f"{url_hash}_{timestamp}"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå›¾ç‰‡å­ç›®å½•
    images_dir = save_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
    downloaded_images = []
    for i, img in enumerate(images, 1):
        local_path = download_image(img['url'], images_dir, i)
        if local_path:
            downloaded_images.append({
                'url': img['url'],
                'local_path': local_path,
                'alt': img['alt']
            })
    
    # ä¿å­˜å†…å®¹ä¸ºæ–‡æœ¬æ–‡ä»¶
    content_file = save_dir / "content.txt"
    with open(content_file, 'w', encoding='utf-8') as f:
        f.write(f"æ ‡é¢˜: {title}\n")
        f.write(f"URL: {url}\n")
        f.write(f"æŠ“å–æ—¶é—´: {datetime.now().isoformat()}\n")
        f.write(f"\n{'='*80}\n\n")
        f.write(content)
    
    # ä¿å­˜å…ƒæ•°æ®ä¸ºJSON
    metadata = {
        'url': url,
        'title': title,
        'crawl_time': datetime.now().isoformat(),
        'content_length': len(content),
        'images_count': len(downloaded_images),
        'images': downloaded_images
    }
    
    metadata_file = save_dir / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    logger.success(f"ä¿å­˜å®Œæˆ! ç›®å½•: {save_dir}")
    logger.info(f"- å†…å®¹æ–‡ä»¶: {content_file}")
    logger.info(f"- å…ƒæ•°æ®: {metadata_file}")
    logger.info(f"- å›¾ç‰‡æ•°é‡: {len(downloaded_images)}")
    
    return save_dir


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python fetch_url.py <URL>")
        print("ç¤ºä¾‹: python fetch_url.py https://www.36kr.com/p/123456")
        sys.exit(1)
    
    url = sys.argv[1]
    
    # éªŒè¯URL
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        logger.error(f"æ— æ•ˆçš„URL: {url}")
        sys.exit(1)
    
    logger.info(f"å¼€å§‹æŠ“å–: {url}")
    
    try:
        # 1. è·å–é¡µé¢
        html, title = get_page_content(url)
        
        # 2. æå–å†…å®¹
        result = extract_content(html, url)
        
        # 3. ä¿å­˜ç»“æœ
        output_dir = Path("data/fetched")
        save_dir = save_results(
            url, 
            title, 
            result['content'], 
            result['images'],
            output_dir
        )
        
        print(f"\nâœ… æŠ“å–æˆåŠŸ!")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {save_dir}")
        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
        print(f"ğŸ–¼ï¸  å›¾ç‰‡æ•°é‡: {len(result['images'])} å¼ ")
        
    except Exception as e:
        logger.error(f"æŠ“å–å¤±è´¥: {e}")
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
