"""
ç‹¬ç«‹çš„VentureBeatæ–‡ç« çˆ¬è™«æµ‹è¯•
æ— éœ€ä¾èµ–é¡¹ç›®å…¶ä»–æ¨¡å—
"""
import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin, urlparse
from pathlib import Path

def crawl_venturebeat_article(url):
    """æŠ“å–VentureBeatæ–‡ç« """
    
    print(f"ğŸš€ å¼€å§‹æŠ“å–VentureBeatæ–‡ç« : {url}")
    print("=" * 60)
    
    # è¯·æ±‚å¤´è®¾ç½®
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        # å‘é€è¯·æ±‚
        print("æ­£åœ¨å‘é€è¯·æ±‚...")
        time.sleep(2)
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print("âœ… è¯·æ±‚æˆåŠŸ!")
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ–‡ç« ä¿¡æ¯
            article_data = {
                'url': url,
                'title': extract_title(soup),
                'author': extract_author(soup),
                'publish_date': extract_publish_date(soup),
                'content': extract_content(soup),
                'images': extract_images(soup, url),
                'tags': extract_tags(soup),
                'summary': extract_summary(soup)
            }
            
            # æ˜¾ç¤ºç»“æœ
            print("\nğŸ“‹ æ–‡ç« ä¿¡æ¯:")
            print(f"æ ‡é¢˜: {article_data['title']}")
            print(f"ä½œè€…: {article_data['author']}")
            print(f"å‘å¸ƒæ—¥æœŸ: {article_data['publish_date']}")
            print(f"å†…å®¹é•¿åº¦: {len(article_data['content'])} å­—ç¬¦")
            print(f"å›¾ç‰‡æ•°é‡: {len(article_data['images'])}")
            print(f"æ ‡ç­¾: {', '.join(article_data['tags']) if article_data['tags'] else 'æ— æ ‡ç­¾'}")
            print(f"æ‘˜è¦: {article_data['summary'][:150]}...")
            
            # æ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
            if article_data['images']:
                print("\nğŸ–¼ï¸  å›¾ç‰‡åˆ—è¡¨:")
                for i, img in enumerate(article_data['images'][:5]):
                    print(f"  {i+1}. {img['alt'][:50]} -> {img['url'][:80]}...")
            
            # ä¿å­˜åŸå§‹æ•°æ®
            with open('venturebeat_article_raw.json', 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜åˆ° venturebeat_article_raw.json")
            
            # ä¸‹è½½å›¾ç‰‡
            downloaded_images = download_article_images(article_data)
            article_data['downloaded_images'] = downloaded_images
            
            # ä¿å­˜å®Œæ•´æ•°æ®
            with open('venturebeat_article_complete.json', 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ’¾ å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ° venturebeat_article_complete.json")
            
            return article_data
            
        else:
            print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
            return None
            
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return None

def extract_title(soup):
    """æå–æ ‡é¢˜"""
    title_elem = soup.find('h1')
    return title_elem.get_text(strip=True) if title_elem else "æœªæ‰¾åˆ°æ ‡é¢˜"

def extract_author(soup):
    """æå–ä½œè€…"""
    author_selectors = ['[class*="author"] a', '[rel="author"]', '.byline-author']
    for selector in author_selectors:
        author_elem = soup.select_one(selector)
        if author_elem:
            return author_elem.get_text(strip=True)
    return "æœªçŸ¥ä½œè€…"

def extract_publish_date(soup):
    """æå–å‘å¸ƒæ—¥æœŸ"""
    time_elem = soup.find('time')
    if time_elem and time_elem.has_attr('datetime'):
        return time_elem['datetime']
    return "æœªçŸ¥æ—¥æœŸ"

def extract_content(soup):
    """æå–æ–‡ç« å†…å®¹"""
    content_selectors = ['[class*="content"]', 'article', '.post-content']
    for selector in content_selectors:
        content_elem = soup.select_one(selector)
        if content_elem:
            # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
            for unwanted in content_elem.select('script, style, .ad, .advertisement, .related-posts, nav'):
                unwanted.decompose()
            content_text = content_elem.get_text(separator='\n', strip=True)
            return content_text[:3000] + "..." if len(content_text) > 3000 else content_text
    return "æœªæ‰¾åˆ°æ–‡ç« å†…å®¹"

def extract_images(soup, base_url):
    """æå–å›¾ç‰‡ä¿¡æ¯"""
    images = []
    img_elements = soup.find_all('img')
    
    for img in img_elements:
        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
        if src:
            full_url = urljoin(base_url, src)
            alt_text = img.get('alt', '')
            images.append({
                'url': full_url,
                'alt': alt_text,
                'title': img.get('title', '')
            })
    return images

def extract_tags(soup):
    """æå–æ ‡ç­¾"""
    tags = []
    tag_selectors = ['.tags a', '.post-tags a', '[rel="tag"]']
    
    for selector in tag_selectors:
        tag_elements = soup.select(selector)
        for tag in tag_elements:
            tag_text = tag.get_text(strip=True)
            if tag_text and tag_text not in tags:
                tags.append(tag_text)
    return tags

def extract_summary(soup):
    """æå–æ‘˜è¦"""
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    if meta_desc:
        return meta_desc.get('content', '')
    return ""

def download_article_images(article_data, output_dir="downloaded_images"):
    """ä¸‹è½½æ–‡ç« å›¾ç‰‡"""
    if not article_data or not article_data.get('images'):
        print("æ²¡æœ‰å›¾ç‰‡éœ€è¦ä¸‹è½½")
        return []
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    images_dir = Path(output_dir) / "venturebeat_final"
    images_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded_images = []
    
    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½å›¾ç‰‡ ({len(article_data['images'])} å¼ )...")
    
    for i, img_info in enumerate(article_data['images']):
        try:
            img_url = img_info['url']
            print(f"ä¸‹è½½å›¾ç‰‡ {i+1}/{len(article_data['images'])}: {img_info['alt'][:30]}...")
            
            # å‘é€è¯·æ±‚ä¸‹è½½å›¾ç‰‡
            response = requests.get(img_url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }, timeout=30)
            
            if response.status_code == 200:
                # ç”Ÿæˆæ–‡ä»¶å
                parsed_url = urlparse(img_url)
                filename = f"article_image_{i+1:03d}_{os.path.basename(parsed_url.path)}"
                if not any(filename.endswith(ext) for ext in ['.jpg', '.png', '.gif', '.webp', '.jpeg']):
                    filename += '.jpg'
                
                filepath = images_dir / filename
                
                # ä¿å­˜å›¾ç‰‡
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                downloaded_images.append(str(filepath))
                file_size = filepath.stat().st_size / 1024  # KB
                print(f"  âœ“ å·²ä¿å­˜ ({file_size:.1f} KB): {filepath.name}")
            else:
                print(f"  âœ— ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            
        except Exception as e:
            print(f"  âœ— ä¸‹è½½å¼‚å¸¸: {e}")
            continue
    
    return downloaded_images

def main():
    """ä¸»å‡½æ•°"""
    # VentureBeatæ–‡ç« URL
    url = "https://venturebeat.com/orchestration/new-agent-framework-matches-human-engineered-ai-systems-and-adds-zero"
    
    # æ‰§è¡Œçˆ¬å–
    article_data = crawl_venturebeat_article(url)
    
    if article_data:
        print("\nğŸ‰ æ–‡ç« æŠ“å–å®Œæˆ!")
        print(f"âœ… æ ‡é¢˜: {article_data['title']}")
        print(f"âœ… ä½œè€…: {article_data['author']}")
        print(f"âœ… å†…å®¹: {len(article_data['content'])} å­—ç¬¦")
        print(f"âœ… å›¾ç‰‡: {len(article_data['images'])} å¼ ")
        print(f"âœ… ä¸‹è½½å›¾ç‰‡: {len(article_data.get('downloaded_images', []))} å¼ ")
    else:
        print("\nâŒ æ–‡ç« æŠ“å–å¤±è´¥!")

if __name__ == "__main__":
    main()