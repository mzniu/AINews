#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•GIFå›¾ç‰‡é‡‡é›†åŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from services.crawler_service import CrawlerService
import asyncio

async def test_gif_collection():
    """æµ‹è¯•GIFå›¾ç‰‡é‡‡é›†åŠŸèƒ½"""
    # æµ‹è¯•URLï¼ˆé€‰æ‹©ä¸€ä¸ªå¯èƒ½åŒ…å«GIFçš„ç½‘ç«™ï¼‰
    test_urls = [
        "https://www.36kr.com/",  # 36æ°ªé¦–é¡µ
        "https://www.qbitai.com/",  # æœºå™¨ä¹‹å¿ƒ
    ]
    
    print("ğŸ” æµ‹è¯•GIFå›¾ç‰‡é‡‡é›†åŠŸèƒ½")
    print("=" * 50)
    
    for url in test_urls:
        print(f"\nğŸŒ æµ‹è¯•ç½‘ç«™: {url}")
        print("-" * 30)
        
        try:
            # 1. è·å–é¡µé¢å†…å®¹
            print("ğŸ“¥ æ­£åœ¨è·å–é¡µé¢å†…å®¹...")
            html, title = await CrawlerService.get_page_content(url)
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title}")
            
            # 2. æå–å†…å®¹å’Œå›¾ç‰‡
            print("\nğŸ–¼ï¸  æ­£åœ¨æå–å›¾ç‰‡...")
            result = CrawlerService.extract_content(html, url)
            
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
            print(f"ğŸ“Š æ€»å›¾ç‰‡æ•°é‡: {len(result['images'])} å¼ ")
            
            # 3. ç»Ÿè®¡å›¾ç‰‡æ ¼å¼
            gif_count = 0
            jpg_count = 0
            png_count = 0
            other_count = 0
            
            for img in result['images']:
                img_url = img.get('url', '')
                if '.gif' in img_url.lower() or 'data:image/gif' in img_url.lower():
                    gif_count += 1
                elif '.jpg' in img_url.lower() or '.jpeg' in img_url.lower():
                    jpg_count += 1
                elif '.png' in img_url.lower():
                    png_count += 1
                else:
                    other_count += 1
            
            print(f"ğŸ“ˆ å›¾ç‰‡æ ¼å¼ç»Ÿè®¡:")
            print(f"   GIFå›¾ç‰‡: {gif_count} å¼ ")
            print(f"   JPGå›¾ç‰‡: {jpg_count} å¼ ")
            print(f"   PNGå›¾ç‰‡: {png_count} å¼ ")
            print(f"   å…¶ä»–æ ¼å¼: {other_count} å¼ ")
            
            # 4. å¦‚æœå‘ç°GIFï¼Œå°è¯•ä¸‹è½½æµ‹è¯•
            if gif_count > 0:
                print(f"\nğŸ¯ å‘ç° {gif_count} å¼ GIFå›¾ç‰‡ï¼Œæ­£åœ¨è¿›è¡Œä¸‹è½½æµ‹è¯•...")
                
                # åªæµ‹è¯•å‰3å¼ GIFå›¾ç‰‡
                gif_images = [img for img in result['images'] 
                            if '.gif' in img.get('url', '').lower() or 'data:image/gif' in img.get('url', '').lower()][:3]
                
                for i, img in enumerate(gif_images):
                    print(f"\n--- æµ‹è¯• GIF {i+1} ---")
                    download_result = CrawlerService.download_image(
                        img['url'], 
                        Path("data/test_gifs"), 
                        i+1, 
                        url
                    )
                    
                    if download_result['success']:
                        print(f"âœ… ä¸‹è½½æˆåŠŸ: {download_result['local_path']}")
                        print(f"   æ ¼å¼: {download_result.get('format', 'Unknown')}")
                    else:
                        print(f"âŒ ä¸‹è½½å¤±è´¥: {download_result['error']}")
                        
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_gif_collection())