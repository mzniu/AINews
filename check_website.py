"""æ£€æŸ¥æœºå™¨ä¹‹å¿ƒç½‘ç«™ç»“æ„"""
import requests
from bs4 import BeautifulSoup
import json

def check_homepage():
    """æ£€æŸ¥é¦–é¡µ"""
    print("=" * 60)
    print("æ£€æŸ¥æœºå™¨ä¹‹å¿ƒé¦–é¡µ")
    print("=" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    try:
        r = requests.get('https://www.jiqizhixin.com/', headers=headers, timeout=15)
        print(f"âœ… çŠ¶æ€ç : {r.status_code}")
        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(r.text)} å­—ç¬¦")
        
        soup = BeautifulSoup(r.text, 'lxml')
        title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
        print(f"ğŸ“Œ é¡µé¢æ ‡é¢˜: {title}")
        
        # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
        links = soup.find_all('a', href=True)
        article_links = [l['href'] for l in links if '/articles/' in str(l.get('href', ''))]
        
        print(f"\nğŸ”— æ‰¾åˆ° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥")
        
        # å»é‡å¹¶æ˜¾ç¤ºå‰10ä¸ª
        unique_links = list(set(article_links))[:10]
        for i, link in enumerate(unique_links, 1):
            if not link.startswith('http'):
                link = 'https://www.jiqizhixin.com' + link
            print(f"  [{i}] {link}")
        
        return unique_links
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return []

def check_article_page(url):
    """æ£€æŸ¥æ–‡ç« é¡µé¢"""
    print("\n" + "=" * 60)
    print(f"æ£€æŸ¥æ–‡ç« é¡µ: {url}")
    print("=" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    
    if not url.startswith('http'):
        url = 'https://www.jiqizhixin.com' + url
    
    try:
        r = requests.get(url, headers=headers, timeout=15)
        print(f"âœ… çŠ¶æ€ç : {r.status_code}")
        
        soup = BeautifulSoup(r.text, 'lxml')
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾æ ‡é¢˜
        title = None
        title_selectors = [
            ('h1', {'class': 'article-title'}),
            ('h1', {}),
            ('div', {'class': 'title'}),
            ('meta', {'property': 'og:title'}),
        ]
        
        for tag, attrs in title_selectors:
            elem = soup.find(tag, attrs)
            if elem:
                if tag == 'meta':
                    title = elem.get('content')
                else:
                    title = elem.get_text(strip=True)
                if title:
                    print(f"ğŸ“Œ æ ‡é¢˜: {title}")
                    break
        
        # æŸ¥æ‰¾å†…å®¹åŒºåŸŸ
        content_selectors = [
            ('div', {'class': 'article-content'}),
            ('article', {}),
            ('div', {'class': 'content'}),
            ('div', {'class': 'post-content'}),
        ]
        
        for tag, attrs in content_selectors:
            elem = soup.find(tag, attrs)
            if elem:
                content = elem.get_text(strip=True)
                print(f"ğŸ“ æ­£æ–‡é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"ğŸ“ æ­£æ–‡é¢„è§ˆ: {content[:100]}...")
                break
        
        # æŸ¥æ‰¾æ‰€æœ‰classåŒ…å«articleæˆ–contentçš„div
        print("\nğŸ” é¡µé¢ä¸­çš„ä¸»è¦å®¹å™¨:")
        for div in soup.find_all('div', class_=True)[:20]:
            classes = ' '.join(div.get('class', []))
            if any(keyword in classes.lower() for keyword in ['article', 'content', 'post', 'main']):
                print(f"  - div.{classes}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    # æ£€æŸ¥é¦–é¡µ
    links = check_homepage()
    
    # æ£€æŸ¥ç¬¬ä¸€ç¯‡æ–‡ç« 
    if links:
        check_article_page(links[0])
