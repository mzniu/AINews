# 爬虫模块设计

## 目标网站列表

### 国内AI资讯网站
1. **机器之心** (jiqizhixin.com)
   - 重点：学术前沿、AI论文解读
   - 更新频率：每日10+篇
   
2. **量子位** (qbitai.com)
   - 重点：产业动态、公司新闻
   - 更新频率：每日5-10篇

3. **AI科技评论** (leiphone.com/category/ai)
   - 重点：技术评测、行业分析
   - 更新频率：每日3-5篇

4. **新智元** (mp.weixin.qq.com/s?__biz=...)
   - 重点：综合AI资讯
   - 更新频率：每日多篇

### 国外AI资讯网站
1. **VentureBeat AI** (venturebeat.com/ai)
2. **The Verge AI** (theverge.com/ai-artificial-intelligence)
3. **TechCrunch AI** (techcrunch.com/tag/artificial-intelligence)

## 爬虫架构

### 1. 通用爬虫框架
```python
class BaseCrawler:
    """爬虫基类"""
    - parse_list()      # 解析列表页
    - parse_detail()    # 解析详情页
    - save_data()       # 保存数据
    - check_duplicate() # 去重检查
```

### 2. 网站适配器
为每个网站创建专门的适配器：
```python
class JiqizhixinCrawler(BaseCrawler):
    """机器之心爬虫"""
    
class QbitaiCrawler(BaseCrawler):
    """量子位爬虫"""
```

## 数据结构

### 原始资讯数据模型
```python
{
    "id": "unique_hash",
    "title": "文章标题",
    "url": "原文链接",
    "source": "来源网站",
    "author": "作者",
    "publish_time": "发布时间",
    "content": "正文内容",
    "summary": "原文摘要（如有）",
    "tags": ["标签1", "标签2"],
    "images": ["图片URL1", "图片URL2"],
    "crawl_time": "抓取时间",
    "status": "处理状态"
}
```

## 去重策略

### 1. URL去重
- 使用URL哈希作为唯一标识
- Redis Set存储已爬取URL

### 2. 内容去重
- 标题相似度检测（编辑距离）
- 内容指纹（SimHash）

### 3. 时间窗口
- 只抓取最近24小时的内容
- 避免重复抓取历史数据

## 反爬策略

### 1. 请求控制
- 随机User-Agent
- 请求延迟（1-3秒）
- IP代理池（如需要）

### 2. 动态内容处理
- Selenium + Chrome Headless
- 等待页面加载完成
- 处理Ajax请求

### 3. 错误处理
- 请求重试机制（最多3次）
- 异常日志记录
- 失败通知

## 爬虫调度

### 执行策略
- **定时任务**: 每2小时执行一次
- **增量爬取**: 只抓取新增内容
- **并发控制**: 控制并发数避免封禁

### 优先级
1. 高优先级：机器之心、量子位（最新AI动态）
2. 中优先级：AI科技评论、新智元
3. 低优先级：国外网站（翻译成本高）

## 数据清洗

### 清洗规则
1. 移除HTML标签
2. 统一换行符
3. 去除广告和推广内容
4. 提取核心正文
5. 标准化时间格式

### 质量过滤
- 文章长度 > 300字
- 包含实质性内容
- 非纯转载文章
