"""
CUDAç¯å¢ƒé…ç½®è„šæœ¬
ç”¨äºé…ç½®æ”¯æŒGPUåŠ é€Ÿçš„PyTorchç¯å¢ƒ
"""

import os
import subprocess
import sys

def check_cuda_availability():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥CUDAç¯å¢ƒ...")
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print(f"âœ… CUDAè®¾å¤‡æ•°: {torch.cuda.device_count()}")
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
        
        return torch.cuda.is_available()
        
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ CUDAæ£€æŸ¥å¤±è´¥: {e}")
        return False

def configure_cuda_environment():
    """é…ç½®CUDAç¯å¢ƒå˜é‡"""
    print("\nğŸ”§ é…ç½®CUDAç¯å¢ƒå˜é‡...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿ä½¿ç”¨GPU
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    print("âœ… å·²è®¾ç½® CUDA_VISIBLE_DEVICES=0")
    print("âœ… å·²è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128")

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    print("\nâš¡ æµ‹è¯•GPUæ€§èƒ½...")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
            return
            
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        device = torch.device('cuda')
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
        
        # æµ‹è¯•çŸ©é˜µè¿ç®—
        size = 1000
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        # é¢„çƒ­
        for _ in range(3):
            c = torch.mm(a, b)
        
        # å®é™…æµ‹è¯•
        import time
        start_time = time.time()
        for _ in range(10):
            c = torch.mm(a, b)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"âœ… çŸ©é˜µä¹˜æ³•æµ‹è¯•å®Œæˆ")
        print(f"   çŸ©é˜µå¤§å°: {size}Ã—{size}")
        print(f"   å¹³å‡è€—æ—¶: {avg_time:.4f} ç§’")
        print(f"   æ€§èƒ½: çº¦ {2 * size**3 / avg_time / 1e9:.1f} GFLOPS")
        
    except Exception as e:
        print(f"âŒ GPUæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

def update_lama_model_config():
    """æ›´æ–°LaMaæ¨¡å‹é…ç½®ä»¥ä½¿ç”¨GPU"""
    print("\nğŸ”„ æ›´æ–°LaMaæ¨¡å‹é…ç½®...")
    
    try:
        # ä¿®æ”¹watermark_routes.pyä¸­çš„get_lama_modelå‡½æ•°
        watermark_file = "api/routes/watermark_routes.py"
        
        if os.path.exists(watermark_file):
            with open(watermark_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›´æ–°è®¾å¤‡é…ç½®
            if "'cpu'" in content:
                # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨åˆ™ä½¿ç”¨GPU
                import torch
                if torch.cuda.is_available():
                    new_content = content.replace("device='cpu'", "device='cuda'")
                    new_content = new_content.replace("'cpu'  # å¼ºåˆ¶ä½¿ç”¨CPU", "'cuda'  # ä½¿ç”¨GPUåŠ é€Ÿ")
                    
                    with open(watermark_file, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                    
                    print("âœ… LaMaæ¨¡å‹é…ç½®å·²æ›´æ–°ä¸ºä½¿ç”¨GPU")
                else:
                    print("â„¹ï¸  CUDAä¸å¯ç”¨ï¼Œä¿æŒCPUé…ç½®")
            else:
                print("âœ… LaMaæ¨¡å‹é…ç½®å·²ç»æ˜¯æœ€æ–°ç‰ˆæœ¬")
        else:
            print("âŒ æ‰¾ä¸åˆ°watermark_routes.pyæ–‡ä»¶")
            
    except Exception as e:
        print(f"âŒ æ›´æ–°LaMaé…ç½®å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ CUDAç¯å¢ƒé…ç½®åŠ©æ‰‹")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAç¯å¢ƒ
    cuda_available = check_cuda_availability()
    
    if not cuda_available:
        print("\nâŒ CUDAç¯å¢ƒæ£€æŸ¥å¤±è´¥")
        print("è¯·ç¡®ä¿ï¼š")
        print("1. å·²å®‰è£…æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬")
        print("2. NVIDIAé©±åŠ¨ç¨‹åºå·²æ­£ç¡®å®‰è£…")
        print("3. GPUè®¾å¤‡æ­£å¸¸å·¥ä½œ")
        return
    
    # é…ç½®ç¯å¢ƒå˜é‡
    configure_cuda_environment()
    
    # æµ‹è¯•GPUæ€§èƒ½
    test_gpu_performance()
    
    # æ›´æ–°æ¨¡å‹é…ç½®
    update_lama_model_config()
    
    print("\nğŸ‰ CUDAç¯å¢ƒé…ç½®å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥äº«å—GPUåŠ é€Ÿçš„å»æ°´å°åŠŸèƒ½äº†ï¼")

if __name__ == "__main__":
    main()