"""
CUDAé…ç½®éªŒè¯è„šæœ¬
éªŒè¯PyTorch CUDAé…ç½®æ˜¯å¦æ­£ç¡®
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def verify_cuda_setup():
    """éªŒè¯CUDAè®¾ç½®"""
    print("ğŸ” CUDAé…ç½®éªŒè¯")
    print("=" * 40)
    
    # 1. æ£€æŸ¥PyTorchç‰ˆæœ¬å’ŒCUDAæ”¯æŒ
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print(f"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}")
        
        if torch.cuda.is_available():
            print(f"âœ… CUDAè®¾å¤‡æ•°: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                props = torch.cuda.get_device_properties(i)
                print(f"   æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
                print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ PyTorchæ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    # 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
    print(f"\nğŸ”§ ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')
    print(f"   CUDA_VISIBLE_DEVICES: {cuda_visible}")
    
    # 3. æµ‹è¯•ç®€å•CUDAæ“ä½œ
    print(f"\nâš¡ CUDAåŠŸèƒ½æµ‹è¯•:")
    if torch.cuda.is_available():
        try:
            # åˆ›å»ºCUDAå¼ é‡
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            
            # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
            z = torch.mm(x, y)
            
            # æ£€æŸ¥ç»“æœ
            if z.is_cuda:
                print("âœ… CUDAå¼ é‡æ“ä½œæ­£å¸¸")
                print(f"âœ… ç»“æœå­˜å‚¨åœ¨: {z.device}")
            else:
                print("âŒ CUDAå¼ é‡æœªåœ¨GPUä¸Š")
                
        except Exception as e:
            print(f"âŒ CUDAæ“ä½œæµ‹è¯•å¤±è´¥: {e}")
            return False
    else:
        print("â„¹ï¸  è·³è¿‡CUDAæ“ä½œæµ‹è¯•ï¼ˆCUDAä¸å¯ç”¨ï¼‰")
    
    # 4. æµ‹è¯•LaMaæ¨¡å‹å¯¼å…¥
    print(f"\nğŸ¨ LaMaæ¨¡å‹æµ‹è¯•:")
    try:
        from api.routes.watermark_routes import get_lama_model
        print("âœ… æ°´å°è·¯ç”±å¯¼å…¥æˆåŠŸ")
        
        # è·å–æ¨¡å‹ï¼ˆè¿™ä¼šè§¦å‘å®é™…åŠ è½½ï¼‰
        model = get_lama_model()
        print(f"âœ… LaMaæ¨¡å‹è·å–æˆåŠŸ")
        print(f"âœ… æ¨¡å‹ç±»å‹: {type(model).__name__}")
        
        # æ£€æŸ¥æ¨¡å‹è®¾å¤‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(model, 'device'):
            print(f"âœ… æ¨¡å‹è®¾å¤‡: {model.device}")
        
    except Exception as e:
        print(f"âŒ LaMaæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print(f"\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•:")
    
    try:
        import torch
        import time
        
        if not torch.cuda.is_available():
            print("â„¹ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½å¯¹æ¯”")
            return
            
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        size = 2000
        iterations = 5
        
        print(f"   æµ‹è¯•çŸ©é˜µå¤§å°: {size}Ã—{size}")
        print(f"   æµ‹è¯•è¿­ä»£æ¬¡æ•°: {iterations}")
        
        # CPUæµ‹è¯•
        print("   æ­£åœ¨æµ‹è¯•CPUæ€§èƒ½...")
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        
        # é¢„çƒ­
        for _ in range(2):
            c = torch.mm(a_cpu, b_cpu)
        
        start_time = time.time()
        for _ in range(iterations):
            c = torch.mm(a_cpu, b_cpu)
        cpu_time = (time.time() - start_time) / iterations
        
        # GPUæµ‹è¯•
        print("   æ­£åœ¨æµ‹è¯•GPUæ€§èƒ½...")
        a_gpu = torch.randn(size, size).cuda()
        b_gpu = torch.randn(size, size).cuda()
        
        # é¢„çƒ­
        for _ in range(2):
            c = torch.mm(a_gpu, b_gpu)
        
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(iterations):
            c = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = (time.time() - start_time) / iterations
        
        # ç»“æœ
        speedup = cpu_time / gpu_time
        print(f"âœ… CPUå¹³å‡è€—æ—¶: {cpu_time:.4f} ç§’")
        print(f"âœ… GPUå¹³å‡è€—æ—¶: {gpu_time:.4f} ç§’")
        print(f"âœ… GPUåŠ é€Ÿæ¯”: {speedup:.1f}x")
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ CUDAé…ç½®éªŒè¯å·¥å…·")
    print("=" * 50)
    
    # éªŒè¯åŸºæœ¬é…ç½®
    setup_ok = verify_cuda_setup()
    
    if setup_ok:
        # è¿›è¡Œæ€§èƒ½æµ‹è¯•
        performance_comparison()
        
        print(f"\nğŸ‰ CUDAé…ç½®éªŒè¯é€šè¿‡ï¼")
        print("âœ… æ‚¨çš„ç³»ç»Ÿå·²å‡†å¤‡å¥½ä½¿ç”¨GPUåŠ é€Ÿçš„å»æ°´å°åŠŸèƒ½")
        print("ğŸ’¡ å»æ°´å°å¤„ç†é€Ÿåº¦å°†æ˜¾è‘—æå‡")
    else:
        print(f"\nâŒ CUDAé…ç½®å­˜åœ¨é—®é¢˜")
        print("è¯·æ£€æŸ¥ï¼š")
        print("1. PyTorch CUDAç‰ˆæœ¬æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("2. NVIDIAé©±åŠ¨ç¨‹åºæ˜¯å¦æœ€æ–°")
        print("3. GPUæ˜¯å¦è¢«å…¶ä»–ç¨‹åºå ç”¨")

if __name__ == "__main__":
    main()