import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin
import time

def analyze_venturebeat_page(url):
    """åˆ†æVentureBeaté¡µé¢ç»“æ„"""
    
    print("ğŸ” åˆ†æVentureBeaté¡µé¢ç»“æ„")
    print("=" * 50)
    print(f"ç›®æ ‡URL: {url}")
    
    # è®¾ç½®è¯·æ±‚å¤´é¿å…è¢«åçˆ¬
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        # æ·»åŠ å»¶è¿Ÿé¿å…è§¦å‘åçˆ¬æœºåˆ¶
        time.sleep(2)
        
        print("æ­£åœ¨è¯·æ±‚é¡µé¢...")
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            print(f"âœ… è¯·æ±‚æˆåŠŸ! çŠ¶æ€ç : {response.status_code}")
            print(f"é¡µé¢å¤§å°: {len(response.content)} å­—èŠ‚")
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # åˆ†æé¡µé¢ç»“æ„
            print("\nğŸ“„ é¡µé¢ç»“æ„åˆ†æ:")
            
            # æŸ¥æ‰¾æ ‡é¢˜
            title_selectors = [
                'h1.article-title',
                'h1.entry-title', 
                'h1.post-title',
                'h1[class*="title"]',
                'title'
            ]
            
            print("æ ‡é¢˜å…ƒç´ :")
            title_found = False
            for selector in title_selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements:
                        print(f"  âœ“ {selector}: {elem.get_text(strip=True)[:100]}...")
                        title_found = True
                        break
                if title_found:
                    break
            
            if not title_found:
                print("  âœ— æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ")
            
            # æŸ¥æ‰¾ä½œè€…ä¿¡æ¯
            print("\nä½œè€…ä¿¡æ¯:")
            author_selectors = [
                '.author-name',
                '.byline-author',
                '[rel="author"]',
                '.post-author a',
                '.entry-author'
            ]
            
            author_found = False
            for selector in author_selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements:
                        print(f"  âœ“ {selector}: {elem.get_text(strip=True)}")
                        author_found = True
                        break
                if author_found:
                    break
            
            if not author_found:
                print("  âœ— æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯")
            
            # æŸ¥æ‰¾å‘å¸ƒæ—¶é—´
            print("\nå‘å¸ƒæ—¶é—´:")
            date_selectors = [
                'time[datetime]',
                '.published',
                '.post-date',
                '.entry-date',
                '[class*="date"]'
            ]
            
            date_found = False
            for selector in date_selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements:
                        date_text = elem.get('datetime') or elem.get_text(strip=True)
                        print(f"  âœ“ {selector}: {date_text}")
                        date_found = True
                        break
                if date_found:
                    break
            
            if not date_found:
                print("  âœ— æœªæ‰¾åˆ°å‘å¸ƒæ—¶é—´")
            
            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            print("\nä¸»è¦å†…å®¹åŒºåŸŸ:")
            content_selectors = [
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                '[class*="content"] article',
                'main article'
            ]
            
            content_found = False
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content_length = len(elements[0].get_text())
                    print(f"  âœ“ {selector}: {content_length} å­—ç¬¦")
                    content_found = True
                    # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹é¢„è§ˆ
                    content_preview = elements[0].get_text()[:200].replace('\n', ' ')
                    print(f"    é¢„è§ˆ: {content_preview}...")
                    break
            
            if not content_found:
                print("  âœ— æœªæ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ")
            
            # æŸ¥æ‰¾å›¾ç‰‡
            print("\nå›¾ç‰‡èµ„æº:")
            img_selectors = [
                '.article-content img',
                '.post-content img',
                '.entry-content img',
                'article img',
                'main img'
            ]
            
            images = []
            for selector in img_selectors:
                img_elements = soup.select(selector)
                if img_elements:
                    print(f"  âœ“ {selector}: æ‰¾åˆ° {len(img_elements)} å¼ å›¾ç‰‡")
                    for i, img in enumerate(img_elements[:3]):  # åªæ˜¾ç¤ºå‰3å¼ 
                        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                        if src:
                            full_url = urljoin(url, src)
                            alt = img.get('alt', 'æ— altæ–‡æœ¬')
                            images.append({'url': full_url, 'alt': alt})
                            print(f"    å›¾ç‰‡{i+1}: {alt[:50]}... -> {full_url}")
                    break
            
            if not images:
                print("  âœ— æœªæ‰¾åˆ°å›¾ç‰‡")
            
            # æŸ¥æ‰¾æ ‡ç­¾
            print("\næ ‡ç­¾ä¿¡æ¯:")
            tag_selectors = [
                '.tags a',
                '.post-tags a',
                '.entry-tags a',
                '[rel="tag"]'
            ]
            
            tags = []
            for selector in tag_selectors:
                tag_elements = soup.select(selector)
                if tag_elements:
                    print(f"  âœ“ {selector}: æ‰¾åˆ° {len(tag_elements)} ä¸ªæ ‡ç­¾")
                    for tag in tag_elements[:5]:
                        tag_text = tag.get_text(strip=True)
                        tags.append(tag_text)
                        print(f"    æ ‡ç­¾: {tag_text}")
                    break
            
            if not tags:
                print("  âœ— æœªæ‰¾åˆ°æ ‡ç­¾")
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            analysis_result = {
                'url': url,
                'title_found': title_found,
                'author_found': author_found,
                'date_found': date_found,
                'content_found': content_found,
                'images_found': len(images),
                'tags_found': len(tags),
                'images': images[:10],  # é™åˆ¶ä¿å­˜çš„å›¾ç‰‡æ•°é‡
                'tags': tags
            }
            
            # ä¿å­˜åˆ†æç»“æœ
            with open('venturebeat_analysis.json', 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ“Š åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° venturebeat_analysis.json")
            
            return analysis_result
            
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥! çŠ¶æ€ç : {response.status_code}")
            return None
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return None

def main():
    url = "https://venturebeat.com/orchestration/new-agent-framework-matches-human-engineered-ai-systems-and-adds-zero"
    result = analyze_venturebeat_page(url)
    
    if result:
        print("\nğŸ¯ åˆ†ææ€»ç»“:")
        print(f"  æ ‡é¢˜æå–: {'âœ“' if result['title_found'] else 'âœ—'}")
        print(f"  ä½œè€…æå–: {'âœ“' if result['author_found'] else 'âœ—'}")
        print(f"  æ—¶é—´æå–: {'âœ“' if result['date_found'] else 'âœ—'}")
        print(f"  å†…å®¹æå–: {'âœ“' if result['content_found'] else 'âœ—'}")
        print(f"  å›¾ç‰‡æ•°é‡: {result['images_found']}")
        print(f"  æ ‡ç­¾æ•°é‡: {result['tags_found']}")

if __name__ == "__main__":
    main()