"""
é€šç”¨æ–‡ç« çˆ¬è™«æ¡†æ¶
æ”¯æŒå¤šç§ç½‘ç«™çš„æ–‡ç« å†…å®¹æŠ“å–
"""
import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin, urlparse
from pathlib import Path
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class ArticleData:
    """æ–‡ç« æ•°æ®ç»“æ„"""
    url: str
    title: str = ""
    author: str = ""
    publish_date: str = ""
    content: str = ""
    images: List[Dict[str, str]] = None
    tags: List[str] = None
    summary: str = ""
    downloaded_images: List[str] = None

class BaseArticleCrawler(ABC):
    """åŸºç¡€æ–‡ç« çˆ¬è™«æŠ½è±¡ç±»"""
    
    def __init__(self, delay: float = 2.0):
        self.delay = delay
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    @abstractmethod
    def can_handle(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†è¯¥URL"""
        pass
    
    @abstractmethod
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ ‡é¢˜"""
        pass
    
    @abstractmethod
    def _extract_author(self, soup: BeautifulSoup) -> str:
        """æå–ä½œè€…"""
        pass
    
    @abstractmethod
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """æå–å†…å®¹"""
        pass
    
    def get_article(self, url: str) -> Optional[ArticleData]:
        """è·å–æ–‡ç« æ•°æ®"""
        try:
            if not self.can_handle(url):
                print(f"ä¸æ”¯æŒçš„ç½‘ç«™: {url}")
                return None
            
            print(f"æ­£åœ¨æŠ“å–: {url}")
            time.sleep(self.delay)
            
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                article_data = ArticleData(
                    url=url,
                    title=self._extract_title(soup),
                    author=self._extract_author(soup),
                    publish_date=self._extract_publish_date(soup),
                    content=self._extract_content(soup),
                    images=self._extract_images(soup, url),
                    tags=self._extract_tags(soup),
                    summary=self._extract_summary(soup)
                )
                
                return article_data
            else:
                print(f"HTTPé”™è¯¯: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {e}")
            return None
    
    def _extract_publish_date(self, soup: BeautifulSoup) -> str:
        """æå–å‘å¸ƒæ—¥æœŸï¼ˆé€šç”¨å®ç°ï¼‰"""
        time_elem = soup.find('time')
        if time_elem and time_elem.has_attr('datetime'):
            return time_elem['datetime']
        return "æœªçŸ¥æ—¥æœŸ"
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """æå–å›¾ç‰‡ä¿¡æ¯ï¼ˆé€šç”¨å®ç°ï¼‰"""
        images = []
        img_elements = soup.find_all('img')
        
        for img in img_elements:
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                full_url = urljoin(base_url, src)
                alt_text = img.get('alt', '')
                images.append({
                    'url': full_url,
                    'alt': alt_text,
                    'title': img.get('title', '')
                })
        return images
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ ‡ç­¾ï¼ˆé€šç”¨å®ç°ï¼‰"""
        tags = []
        tag_selectors = ['.tags a', '.post-tags a', '[rel="tag"]']
        
        for selector in tag_selectors:
            tag_elements = soup.select(selector)
            for tag in tag_elements:
                tag_text = tag.get_text(strip=True)
                if tag_text and tag_text not in tags:
                    tags.append(tag_text)
        return tags
    
    def _extract_summary(self, soup: BeautifulSoup) -> str:
        """æå–æ‘˜è¦ï¼ˆé€šç”¨å®ç°ï¼‰"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        return ""

class VentureBeatCrawler(BaseArticleCrawler):
    """VentureBeatä¸“ç”¨çˆ¬è™«"""
    
    def can_handle(self, url: str) -> bool:
        return 'venturebeat.com' in url
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        title_elem = soup.find('h1')
        return title_elem.get_text(strip=True) if title_elem else "æœªæ‰¾åˆ°æ ‡é¢˜"
    
    def _extract_author(self, soup: BeautifulSoup) -> str:
        author_selectors = ['[class*="author"] a', '[rel="author"]', '.byline-author']
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                return author_elem.get_text(strip=True)
        return "æœªçŸ¥ä½œè€…"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        content_selectors = ['[class*="content"]', 'article', '.post-content']
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in content_elem.select('script, style, .ad, .advertisement, .related-posts, nav'):
                    unwanted.decompose()
                content_text = content_elem.get_text(separator='\n', strip=True)
                return content_text[:2000] + "..." if len(content_text) > 2000 else content_text
        return "æœªæ‰¾åˆ°æ–‡ç« å†…å®¹"

class GenericCrawler(BaseArticleCrawler):
    """é€šç”¨çˆ¬è™«ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ç½‘ç«™"""
    
    def can_handle(self, url: str) -> bool:
        return True  # é€šç”¨çˆ¬è™«å¯ä»¥å¤„ç†ä»»ä½•URL
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = ['h1', 'h1[class*="title"]', 'title']
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                return title_elem.get_text(strip=True)
        return "æœªæ‰¾åˆ°æ ‡é¢˜"
    
    def _extract_author(self, soup: BeautifulSoup) -> str:
        author_selectors = [
            '[rel="author"]', 
            '.author-name',
            '.byline-author a',
            '[class*="author"]'
        ]
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                return author_elem.get_text(strip=True)
        return "æœªçŸ¥ä½œè€…"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        content_selectors = [
            'article',
            '[class*="content"]',
            '.post-content',
            '.entry-content',
            'main'
        ]
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in content_elem.select('script, style, .ad, .advertisement, .related-posts, nav, header, footer'):
                    unwanted.decompose()
                content_text = content_elem.get_text(separator='\n', strip=True)
                return content_text[:3000] + "..." if len(content_text) > 3000 else content_text
        return "æœªæ‰¾åˆ°æ–‡ç« å†…å®¹"

class ArticleCrawlerManager:
    """æ–‡ç« çˆ¬è™«ç®¡ç†å™¨"""
    
    def __init__(self):
        self.crawlers = [
            VentureBeatCrawler(),
            GenericCrawler()
        ]
    
    def crawl_article(self, url: str) -> Optional[ArticleData]:
        """æ ¹æ®URLé€‰æ‹©åˆé€‚çš„çˆ¬è™«"""
        for crawler in self.crawlers:
            if crawler.can_handle(url):
                print(f"ä½¿ç”¨çˆ¬è™«: {crawler.__class__.__name__}")
                return crawler.get_article(url)
        
        print("æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„çˆ¬è™«")
        return None
    
    def download_images(self, article_data: ArticleData, output_dir: str = "downloaded_images") -> List[str]:
        """ä¸‹è½½æ–‡ç« å›¾ç‰‡"""
        if not article_data or not article_data.images:
            return []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        domain = urlparse(article_data.url).netloc
        images_dir = Path(output_dir) / domain.replace('.', '_')
        images_dir.mkdir(parents=True, exist_ok=True)
        
        downloaded_images = []
        
        print(f"å¼€å§‹ä¸‹è½½ {len(article_data.images)} å¼ å›¾ç‰‡...")
        
        for i, img_info in enumerate(article_data.images):
            try:
                img_url = img_info['url']
                print(f"ä¸‹è½½å›¾ç‰‡ {i+1}/{len(article_data.images)}: {img_url}")
                
                response = requests.get(img_url, headers=self.crawlers[0].headers, timeout=30)
                
                if response.status_code == 200:
                    # ç”Ÿæˆæ–‡ä»¶å
                    parsed_url = urlparse(img_url)
                    filename = f"image_{i+1:03d}_{os.path.basename(parsed_url.path)}"
                    if not any(filename.endswith(ext) for ext in ['.jpg', '.png', '.gif', '.webp', '.jpeg']):
                        filename += '.jpg'
                    
                    filepath = images_dir / filename
                    
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    
                    downloaded_images.append(str(filepath))
                    print(f"  âœ“ å·²ä¿å­˜: {filepath}")
                else:
                    print(f"  âœ— ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    
                time.sleep(1)
                
            except Exception as e:
                print(f"  âœ— ä¸‹è½½å¼‚å¸¸: {e}")
                continue
        
        return downloaded_images

def main():
    """æµ‹è¯•å‡½æ•°"""
    urls = [
        "https://venturebeat.com/orchestration/new-agent-framework-matches-human-engineered-ai-systems-and-adds-zero",
        # å¯ä»¥æ·»åŠ å…¶ä»–ç½‘ç«™çš„URLè¿›è¡Œæµ‹è¯•
    ]
    
    manager = ArticleCrawlerManager()
    
    for url in urls:
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å¤„ç†: {url}")
        print('='*60)
        
        # æŠ“å–æ–‡ç« 
        article_data = manager.crawl_article(url)
        
        if article_data:
            print("âœ… æ–‡ç« æŠ“å–æˆåŠŸ!")
            print(f"æ ‡é¢˜: {article_data.title}")
            print(f"ä½œè€…: {article_data.author}")
            print(f"å‘å¸ƒæ—¥æœŸ: {article_data.publish_date}")
            print(f"å†…å®¹é•¿åº¦: {len(article_data.content)} å­—ç¬¦")
            print(f"å›¾ç‰‡æ•°é‡: {len(article_data.images)}")
            print(f"æ ‡ç­¾: {', '.join(article_data.tags)}")
            print(f"æ‘˜è¦: {article_data.summary[:100]}...")
            
            # ä¸‹è½½å›¾ç‰‡
            downloaded_images = manager.download_images(article_data)
            article_data.downloaded_images = downloaded_images
            print(f"ğŸ–¼ï¸  æˆåŠŸä¸‹è½½ {len(downloaded_images)} å¼ å›¾ç‰‡")
            
            # ä¿å­˜æ•°æ®
            filename = f"article_{urlparse(url).netloc.replace('.', '_')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(article_data.__dict__, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        else:
            print("âŒ æ–‡ç« æŠ“å–å¤±è´¥")

if __name__ == "__main__":
    main()