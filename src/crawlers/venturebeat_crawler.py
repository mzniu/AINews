"""
VentureBeatæ–‡ç« é¡µé¢çˆ¬è™«
ç”¨äºæŠ“å–æ–‡ç« å†…å®¹å’Œå›¾ç‰‡
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import json
from pathlib import Path
from urllib.parse import urljoin, urlparse
import time
from loguru import logger

class VentureBeatCrawler:
    """VentureBeatç½‘ç«™çˆ¬è™«"""
    
    def __init__(self, delay: float = 2.0):
        self.delay = delay
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=self.headers
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> str:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            await asyncio.sleep(self.delay)  # è¯·æ±‚é—´éš”
            
            logger.info(f"æ­£åœ¨æŠ“å–: {url}")
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    logger.info(f"æˆåŠŸè·å–é¡µé¢ï¼Œå¤§å°: {len(content)} å­—ç¬¦")
                    return content
                else:
                    logger.error(f"HTTPé”™è¯¯: {response.status}")
                    return ""
        except Exception as e:
            logger.error(f"æŠ“å–é¡µé¢å¤±è´¥: {e}")
            return ""
    
    def parse_article_content(self, html_content: str, base_url: str) -> dict:
        """è§£ææ–‡ç« å†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        result = {
            'title': '',
            'author': '',
            'publish_date': '',
            'content': '',
            'images': [],
            'tags': []
        }
        
        # æå–æ ‡é¢˜
        title_selectors = [
            'h1.article-title',
            'h1.entry-title',
            'h1.post-title',
            'h1[class*="title"]',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                result['title'] = title_elem.get_text(strip=True)
                break
        
        # æå–ä½œè€…
        author_selectors = [
            '.author-name',
            '.byline-author',
            '[rel="author"]',
            '.post-author a',
            '.entry-author'
        ]
        
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                result['author'] = author_elem.get_text(strip=True)
                break
        
        # æå–å‘å¸ƒæ—¥æœŸ
        date_selectors = [
            'time[datetime]',
            '.published',
            '.post-date',
            '.entry-date',
            '[class*="date"]'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                if date_elem.has_attr('datetime'):
                    result['publish_date'] = date_elem['datetime']
                else:
                    result['publish_date'] = date_elem.get_text(strip=True)
                break
        
        # æå–ä¸»è¦å†…å®¹
        content_selectors = [
            '.article-content',
            '.post-content',
            '.entry-content',
            '.content',
            '[class*="content"] article',
            'main article'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in content_elem.select('script, style, .ad, .advertisement, .related-posts'):
                    unwanted.decompose()
                
                result['content'] = content_elem.get_text(strip=True, separator='\n')
                break
        
        # æå–å›¾ç‰‡
        img_selectors = [
            '.article-content img',
            '.post-content img',
            '.entry-content img',
            'article img',
            'main img'
        ]
        
        for selector in img_selectors:
            img_elements = soup.select(selector)
            for img in img_elements:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                if src:
                    full_url = urljoin(base_url, src)
                    alt_text = img.get('alt', '')
                    result['images'].append({
                        'url': full_url,
                        'alt': alt_text,
                        'title': img.get('title', '')
                    })
        
        # æå–æ ‡ç­¾
        tag_selectors = [
            '.tags a',
            '.post-tags a',
            '.entry-tags a',
            '[rel="tag"]'
        ]
        
        for selector in tag_selectors:
            tag_elements = soup.select(selector)
            for tag in tag_elements:
                tag_text = tag.get_text(strip=True)
                if tag_text and tag_text not in result['tags']:
                    result['tags'].append(tag_text)
        
        return result
    
    async def download_image(self, image_url: str, save_path: Path) -> bool:
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            async with self.session.get(image_url) as response:
                if response.status == 200:
                    save_path.parent.mkdir(parents=True, exist_ok=True)
                    with open(save_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            f.write(chunk)
                    logger.info(f"å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {save_path}")
                    return True
                else:
                    logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ {response.status}: {image_url}")
                    return False
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¼‚å¸¸: {e}")
            return False
    
    async def crawl_article(self, url: str, download_images: bool = True) -> dict:
        """çˆ¬å–å®Œæ•´æ–‡ç« """
        try:
            # è·å–é¡µé¢å†…å®¹
            html_content = await self.fetch_page(url)
            if not html_content:
                return {}
            
            # è§£ææ–‡ç« å†…å®¹
            article_data = self.parse_article_content(html_content, url)
            
            # ä¸‹è½½å›¾ç‰‡
            if download_images and article_data.get('images'):
                logger.info(f"å¼€å§‹ä¸‹è½½ {len(article_data['images'])} å¼ å›¾ç‰‡")
                images_dir = Path("downloaded_images") / urlparse(url).netloc
                downloaded_images = []
                
                for i, img_info in enumerate(article_data['images']):
                    try:
                        img_filename = f"image_{i+1:03d}_{Path(urlparse(img_info['url']).path).name}"
                        img_path = images_dir / img_filename
                        
                        if await self.download_image(img_info['url'], img_path):
                            downloaded_images.append(str(img_path))
                            
                    except Exception as e:
                        logger.error(f"å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")
                        continue
                
                article_data['downloaded_images'] = downloaded_images
            
            return article_data
            
        except Exception as e:
            logger.error(f"çˆ¬å–æ–‡ç« å¤±è´¥: {e}")
            return {}

# æµ‹è¯•å‡½æ•°
async def test_venturebeat_crawler():
    """æµ‹è¯•VentureBeatçˆ¬è™«"""
    url = "https://venturebeat.com/orchestration/new-agent-framework-matches-human-engineered-ai-systems-and-adds-zero"
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•VentureBeatçˆ¬è™«")
    print("=" * 50)
    
    async with VentureBeatCrawler(delay=3.0) as crawler:
        article_data = await crawler.crawl_article(url, download_images=True)
        
        if article_data:
            print("âœ… æ–‡ç« æŠ“å–æˆåŠŸ!")
            print(f"æ ‡é¢˜: {article_data.get('title', 'N/A')}")
            print(f"ä½œè€…: {article_data.get('author', 'N/A')}")
            print(f"å‘å¸ƒæ—¶é—´: {article_data.get('publish_date', 'N/A')}")
            print(f"å†…å®¹é•¿åº¦: {len(article_data.get('content', ''))} å­—ç¬¦")
            print(f"å›¾ç‰‡æ•°é‡: {len(article_data.get('images', []))}")
            print(f"ä¸‹è½½å›¾ç‰‡: {len(article_data.get('downloaded_images', []))}")
            print(f"æ ‡ç­¾: {', '.join(article_data.get('tags', []))}")
            
            # ä¿å­˜ç»“æœ
            output_file = Path("venturebeat_article.json")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2)
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
        else:
            print("âŒ æ–‡ç« æŠ“å–å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(test_venturebeat_crawler())