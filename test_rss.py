"""ä½¿ç”¨RSSè®¢é˜…æºè·å–AIèµ„è®¯"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import feedparser
import requests
from datetime import datetime
from src.models.article import Article
from src.utils.logger import logger
import json

# å¸¸è§çš„AIèµ„è®¯RSSæº
RSS_SOURCES = {
    "æœºå™¨ä¹‹å¿ƒRSS": "https://www.jiqizhixin.com/rss",
    "é‡å­ä½RSS": "https://www.qbitai.com/feed",
    "AIç§‘æŠ€è¯„è®ºRSS": "https://www.leiphone.com/category/ai/feed",
    "36æ°ªAI": "https://36kr.com/feed/ai",
}

def fetch_rss(name, url, max_articles=10):
    """è·å–RSSè®¢é˜…"""
    logger.info(f"æ­£åœ¨è·å– {name}: {url}")
    
    try:
        # ä½¿ç”¨requestsè·å–ï¼Œé¿å…feedparserçš„ç½‘ç»œé—®é¢˜
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            logger.warning(f"{name} è¿”å›çŠ¶æ€ç : {response.status_code}")
            return []
        
        # è§£æRSS
        feed = feedparser.parse(response.content)
        logger.success(f"âœ… {name} - æ‰¾åˆ° {len(feed.entries)} æ¡èµ„è®¯")
        
        articles = []
        for entry in feed.entries[:max_articles]:
            try:
                article = Article(
                    id=entry.get('id', entry.link),
                    title=entry.get('title', ''),
                    url=entry.get('link', ''),
                    source=name,
                    author=entry.get('author', ''),
                    publish_time=datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else None,
                    content=entry.get('summary', entry.get('description', '')),
                    summary=entry.get('summary', '')[:200],
                    tags=[tag.term for tag in entry.get('tags', [])],
                    images=[]
                )
                articles.append(article)
                logger.info(f"  - {article.title}")
            except Exception as e:
                logger.error(f"è§£ææ¡ç›®å¤±è´¥: {e}")
                continue
        
        return articles
        
    except requests.Timeout:
        logger.warning(f"âŒ {name} è¶…æ—¶")
    except Exception as e:
        logger.error(f"âŒ {name} é”™è¯¯: {e}")
    
    return []

def main():
    """ä¸»å‡½æ•°"""
    all_articles = []
    
    for name, url in RSS_SOURCES.items():
        articles = fetch_rss(name, url, max_articles=5)
        all_articles.extend(articles)
    
    if all_articles:
        logger.success(f"\nğŸ“Š æ€»å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
        
        # ä¿å­˜åˆ°JSON
        output_dir = Path("data/raw/rss")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"articles_{timestamp}.json"
        
        data = [article.to_dict() for article in all_articles]
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.success(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    else:
        logger.warning("æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")

if __name__ == "__main__":
    main()
